{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3f2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvisionNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading torchvision-0.11.3-cp36-cp36m-win_amd64.whl (985 kB)\n",
      "Collecting torch==1.10.2\n",
      "  Downloading torch-1.10.2-cp36-cp36m-win_amd64.whl (226.6 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\pravi\\anaconda3\\envs\\deepl2\\lib\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\pravi\\anaconda3\\envs\\deepl2\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pravi\\anaconda3\\envs\\deepl2\\lib\\site-packages (from torch==1.10.2->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\pravi\\anaconda3\\envs\\deepl2\\lib\\site-packages (from torch==1.10.2->torchvision) (0.8)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.10.2 torchvision-0.11.3\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b3fa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pravi\\anaconda3\\envs\\deepl2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e077a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979b7269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75a894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3958dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:07, 1414166.47it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 2243287.49it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:02, 605639.89it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 3395767.94it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='data',train = True,\n",
    "                                          transform = transforms.ToTensor(),\n",
    "                                          download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad7582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='data',train=False,\n",
    "                                         transform = transforms.ToTensor(),\n",
    "                                         download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc91b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f6fd1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25438d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54406a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2959fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a224b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples ,labels = examples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85331f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cc333f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0371b3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(samples[i][0],cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1511f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab1e12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size,hidden_size,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc52f0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (l1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (l2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f82c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cc9e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20, step 100/600,loss = 0.4329\n",
      "epoch 1/20, step 200/600,loss = 0.3376\n",
      "epoch 1/20, step 300/600,loss = 0.1830\n",
      "epoch 1/20, step 400/600,loss = 0.3021\n",
      "epoch 1/20, step 500/600,loss = 0.3065\n",
      "epoch 1/20, step 600/600,loss = 0.2782\n",
      "epoch 2/20, step 100/600,loss = 0.1614\n",
      "epoch 2/20, step 200/600,loss = 0.0884\n",
      "epoch 2/20, step 300/600,loss = 0.2241\n",
      "epoch 2/20, step 400/600,loss = 0.2860\n",
      "epoch 2/20, step 500/600,loss = 0.1787\n",
      "epoch 2/20, step 600/600,loss = 0.2223\n",
      "epoch 3/20, step 100/600,loss = 0.1006\n",
      "epoch 3/20, step 200/600,loss = 0.2196\n",
      "epoch 3/20, step 300/600,loss = 0.2500\n",
      "epoch 3/20, step 400/600,loss = 0.1005\n",
      "epoch 3/20, step 500/600,loss = 0.0686\n",
      "epoch 3/20, step 600/600,loss = 0.0835\n",
      "epoch 4/20, step 100/600,loss = 0.0561\n",
      "epoch 4/20, step 200/600,loss = 0.1281\n",
      "epoch 4/20, step 300/600,loss = 0.1767\n",
      "epoch 4/20, step 400/600,loss = 0.0838\n",
      "epoch 4/20, step 500/600,loss = 0.0718\n",
      "epoch 4/20, step 600/600,loss = 0.2172\n",
      "epoch 5/20, step 100/600,loss = 0.1347\n",
      "epoch 5/20, step 200/600,loss = 0.1180\n",
      "epoch 5/20, step 300/600,loss = 0.1013\n",
      "epoch 5/20, step 400/600,loss = 0.0436\n",
      "epoch 5/20, step 500/600,loss = 0.0627\n",
      "epoch 5/20, step 600/600,loss = 0.0623\n",
      "epoch 6/20, step 100/600,loss = 0.0481\n",
      "epoch 6/20, step 200/600,loss = 0.1054\n",
      "epoch 6/20, step 300/600,loss = 0.0528\n",
      "epoch 6/20, step 400/600,loss = 0.1849\n",
      "epoch 6/20, step 500/600,loss = 0.1201\n",
      "epoch 6/20, step 600/600,loss = 0.0783\n",
      "epoch 7/20, step 100/600,loss = 0.0394\n",
      "epoch 7/20, step 200/600,loss = 0.0266\n",
      "epoch 7/20, step 300/600,loss = 0.0901\n",
      "epoch 7/20, step 400/600,loss = 0.0637\n",
      "epoch 7/20, step 500/600,loss = 0.0525\n",
      "epoch 7/20, step 600/600,loss = 0.0194\n",
      "epoch 8/20, step 100/600,loss = 0.0131\n",
      "epoch 8/20, step 200/600,loss = 0.0465\n",
      "epoch 8/20, step 300/600,loss = 0.0684\n",
      "epoch 8/20, step 400/600,loss = 0.0183\n",
      "epoch 8/20, step 500/600,loss = 0.0278\n",
      "epoch 8/20, step 600/600,loss = 0.0307\n",
      "epoch 9/20, step 100/600,loss = 0.0583\n",
      "epoch 9/20, step 200/600,loss = 0.1203\n",
      "epoch 9/20, step 300/600,loss = 0.0298\n",
      "epoch 9/20, step 400/600,loss = 0.0529\n",
      "epoch 9/20, step 500/600,loss = 0.0761\n",
      "epoch 9/20, step 600/600,loss = 0.0692\n",
      "epoch 10/20, step 100/600,loss = 0.0221\n",
      "epoch 10/20, step 200/600,loss = 0.0302\n",
      "epoch 10/20, step 300/600,loss = 0.0355\n",
      "epoch 10/20, step 400/600,loss = 0.0329\n",
      "epoch 10/20, step 500/600,loss = 0.0511\n",
      "epoch 10/20, step 600/600,loss = 0.0593\n",
      "epoch 11/20, step 100/600,loss = 0.0137\n",
      "epoch 11/20, step 200/600,loss = 0.0167\n",
      "epoch 11/20, step 300/600,loss = 0.0543\n",
      "epoch 11/20, step 400/600,loss = 0.0263\n",
      "epoch 11/20, step 500/600,loss = 0.0476\n",
      "epoch 11/20, step 600/600,loss = 0.0253\n",
      "epoch 12/20, step 100/600,loss = 0.0275\n",
      "epoch 12/20, step 200/600,loss = 0.0092\n",
      "epoch 12/20, step 300/600,loss = 0.0188\n",
      "epoch 12/20, step 400/600,loss = 0.0452\n",
      "epoch 12/20, step 500/600,loss = 0.0154\n",
      "epoch 12/20, step 600/600,loss = 0.0142\n",
      "epoch 13/20, step 100/600,loss = 0.0149\n",
      "epoch 13/20, step 200/600,loss = 0.0575\n",
      "epoch 13/20, step 300/600,loss = 0.0588\n",
      "epoch 13/20, step 400/600,loss = 0.0189\n",
      "epoch 13/20, step 500/600,loss = 0.0239\n",
      "epoch 13/20, step 600/600,loss = 0.0096\n",
      "epoch 14/20, step 100/600,loss = 0.0110\n",
      "epoch 14/20, step 200/600,loss = 0.0160\n",
      "epoch 14/20, step 300/600,loss = 0.0097\n",
      "epoch 14/20, step 400/600,loss = 0.0201\n",
      "epoch 14/20, step 500/600,loss = 0.0038\n",
      "epoch 14/20, step 600/600,loss = 0.0745\n",
      "epoch 15/20, step 100/600,loss = 0.0060\n",
      "epoch 15/20, step 200/600,loss = 0.0189\n",
      "epoch 15/20, step 300/600,loss = 0.0159\n",
      "epoch 15/20, step 400/600,loss = 0.0114\n",
      "epoch 15/20, step 500/600,loss = 0.0479\n",
      "epoch 15/20, step 600/600,loss = 0.0059\n",
      "epoch 16/20, step 100/600,loss = 0.0087\n",
      "epoch 16/20, step 200/600,loss = 0.0333\n",
      "epoch 16/20, step 300/600,loss = 0.0293\n",
      "epoch 16/20, step 400/600,loss = 0.0083\n",
      "epoch 16/20, step 500/600,loss = 0.0194\n",
      "epoch 16/20, step 600/600,loss = 0.0201\n",
      "epoch 17/20, step 100/600,loss = 0.0133\n",
      "epoch 17/20, step 200/600,loss = 0.0101\n",
      "epoch 17/20, step 300/600,loss = 0.0346\n",
      "epoch 17/20, step 400/600,loss = 0.0050\n",
      "epoch 17/20, step 500/600,loss = 0.0398\n",
      "epoch 17/20, step 600/600,loss = 0.0066\n",
      "epoch 18/20, step 100/600,loss = 0.0067\n",
      "epoch 18/20, step 200/600,loss = 0.0009\n",
      "epoch 18/20, step 300/600,loss = 0.0073\n",
      "epoch 18/20, step 400/600,loss = 0.0038\n",
      "epoch 18/20, step 500/600,loss = 0.0092\n",
      "epoch 18/20, step 600/600,loss = 0.0218\n",
      "epoch 19/20, step 100/600,loss = 0.0026\n",
      "epoch 19/20, step 200/600,loss = 0.0152\n",
      "epoch 19/20, step 300/600,loss = 0.0067\n",
      "epoch 19/20, step 400/600,loss = 0.0129\n",
      "epoch 19/20, step 500/600,loss = 0.0211\n",
      "epoch 19/20, step 600/600,loss = 0.0279\n",
      "epoch 20/20, step 100/600,loss = 0.0074\n",
      "epoch 20/20, step 200/600,loss = 0.0163\n",
      "epoch 20/20, step 300/600,loss = 0.0166\n",
      "epoch 20/20, step 400/600,loss = 0.0025\n",
      "epoch 20/20, step 500/600,loss = 0.0044\n",
      "epoch 20/20, step 600/600,loss = 0.0196\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        #forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        #backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps},loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d923d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 97.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "    _, predictions = torch.max(outputs,1)\n",
    "    n_samples += labels.shape[0]\n",
    "    n_correct += (predictions ==  labels).sum().item()\n",
    "    acc = 100.0 * n_correct/n_samples\n",
    "    print(f\"accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11923a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f931dba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae4b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
